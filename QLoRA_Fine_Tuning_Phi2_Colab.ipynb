{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxa69W7Xrng_"
      },
      "source": [
        "# QLoRA Fine-Tuning on Google Colab (Phi-2 + DialogSum or your custom CSV)\n",
        "\n",
        "This notebook fine-tunes a **2.7B**-parameter base model (**microsoft/phi-2**) using **QLoRA** (4-bit quantization + LoRA adapters) with Hugging Face **Transformers**, **PEFT**, **bitsandbytes**, **Accelerate**, **TRL**, and **datasets**.\n",
        "\n",
        "You can:\n",
        "1. Use the sample dataset (**DialogSum**), or\n",
        "2. Upload your **own CSV** with columns like `prompt,response` *or* `instruction,input,output`.\n",
        "\n",
        "**Before running:**\n",
        "- In Colab, go to **Runtime → Change runtime type → GPU**.\n",
        "- Make sure you have a Hugging Face account and have **accepted the model license** for `microsoft/phi-2` on the Hub.\n",
        "- Keep your HF access token handy.\n"
      ],
      "id": "Nxa69W7Xrng_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvidia_smi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70c9171f-449c-4efe-de88-ba408699edaf"
      },
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
      ],
      "id": "nvidia_smi",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "installs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c344a332-1483-4609-dade-caf9f8d32500"
      },
      "source": [
        "%%bash\n",
        "pip -q install -U bitsandbytes==0.43.3 transformers==4.43.3 peft==0.11.1 accelerate==0.33.0 datasets==2.19.1 einops==0.8.0 scipy evaluate rouge_score trl==0.9.6 sentencepiece huggingface_hub"
      ],
      "id": "installs",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.7/43.7 kB 1.6 MB/s eta 0:00:00\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.0/61.0 kB 3.8 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.5/137.5 MB 8.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/9.4 MB 131.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.6/251.6 kB 21.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 315.1/315.1 kB 27.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 38.5 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.2/43.2 kB 3.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 22.4 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.1/84.1 kB 7.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 16.1 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.0/18.0 MB 70.6 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 83.3 MB/s eta 0:00:00\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.7/131.7 kB 11.0 MB/s eta 0:00:00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, math, random\n",
        "import torch\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
        "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
        "    GenerationConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from huggingface_hub import login\n",
        "from functools import partial\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print('Torch:', torch.__version__)"
      ],
      "metadata": {
        "id": "B-_1t19cvmnP"
      },
      "id": "B-_1t19cvmnP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "login"
      },
      "source": [
        "# 🔐 Login to Hugging Face so the notebook can download gated models (like microsoft/phi-2)\n",
        "login()  # Paste your HF token when prompted"
      ],
      "id": "login",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "config"
      },
      "source": [
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = 'microsoft/phi-2'  # base model\n",
        "\n",
        "# Option A: use the DialogSum dataset from HF\n",
        "USE_HF_DATASET = True\n",
        "HF_DATASET_NAME = 'neil-code/dialogsum-test'  # small sample version for quick runs\n",
        "\n",
        "# Option B: upload your CSV in Colab (Runtime ▶ Run all will pause at upload cell later)\n",
        "# Expected columns for custom CSV:\n",
        "#   - Either: prompt,response\n",
        "#   - Or: instruction,input,output\n",
        "\n",
        "# Training hyperparameters (safe defaults for a T4)\n",
        "TRAIN_STEPS = 400  # increase later (e.g., 1000+) for better results\n",
        "PER_DEVICE_BATCH = 1\n",
        "GRAD_ACCUM = 4\n",
        "LR = 2e-4\n",
        "MAX_NEW_TOKENS = 128\n",
        "SEED = 42\n",
        "OUTPUT_DIR = f\"./peft-qlora-{int(time.time())}\"\n",
        "os.environ['WANDB_DISABLED'] = 'true'"
      ],
      "id": "config",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dataset"
      },
      "source": [
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "if USE_HF_DATASET:\n",
        "    ds = load_dataset(HF_DATASET_NAME)\n",
        "    # Expect: columns: dialogue, summary, topic, id\n",
        "else:\n",
        "    # Upload CSV via Colab UI\n",
        "    try:\n",
        "        from google.colab import files  # type: ignore\n",
        "        uploaded = files.upload()\n",
        "        csv_path = list(uploaded.keys())[0]\n",
        "    except Exception as e:\n",
        "        raise RuntimeError('Upload your CSV first or set USE_HF_DATASET=True')\n",
        "    df = pd.read_csv(csv_path)\n",
        "    # Normalize to a common schema similar to DialogSum\n",
        "    if set(['prompt','response']).issubset(df.columns):\n",
        "        df = df.rename(columns={'prompt':'dialogue','response':'summary'})\n",
        "    elif set(['instruction','input','output']).issubset(df.columns):\n",
        "        # Join instruction + input into one 'dialogue' field\n",
        "        df['dialogue'] = df['instruction'].fillna('') + '\\n' + df['input'].fillna('')\n",
        "        df['summary'] = df['output']\n",
        "    else:\n",
        "        raise ValueError('CSV must have either (prompt,response) or (instruction,input,output) columns.')\n",
        "    df = df[['dialogue','summary']].dropna().reset_index(drop=True)\n",
        "    # 90/10 split, then 50/50 split of the 10% for val/test\n",
        "    n = len(df)\n",
        "    train_df = df.sample(frac=0.9, random_state=SEED)\n",
        "    rest_df = df.drop(train_df.index)\n",
        "    val_df = rest_df.sample(frac=0.5, random_state=SEED)\n",
        "    test_df = rest_df.drop(val_df.index)\n",
        "    ds = DatasetDict({\n",
        "        'train': Dataset.from_pandas(train_df),\n",
        "        'validation': Dataset.from_pandas(val_df),\n",
        "        'test': Dataset.from_pandas(test_df)\n",
        "    })\n",
        "\n",
        "print(ds)"
      ],
      "id": "dataset",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of below cell:\n",
        "Choose compute precision → Uses bfloat16 (on GPUs that support it) or float16 (fallback).\n",
        "\n",
        "Setup quantization config with BitsAndBytesConfig:\n",
        "\n",
        "Load model in 4-bit precision (saves VRAM).\n",
        "\n",
        "Use special quantization type \"nf4\".\n",
        "\n",
        "No double quantization (keeps things simple).\n",
        "\n",
        "Load model (AutoModelForCausalLM) with quantization and device_map='auto' so HuggingFace automatically spreads model layers across available GPU(s).\n",
        "\n",
        "Load tokenizer → matches the model and sets padding.\n",
        "\n",
        "If tokenizer doesn’t have a padding token, it uses the end-of-sequence token (eos).\n",
        "\n",
        "Final print confirms successful 4-bit model load.\n",
        "\n",
        "In short: This cell loads the base language model (phi-2) in memory-efficient 4-bit precision using bitsandbytes, and prepares the tokenizer for text processing.\n"
      ],
      "metadata": {
        "id": "RRF0DzbqAqUG"
      },
      "id": "RRF0DzbqAqUG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnb_model"
      },
      "source": [
        "compute_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float16\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "device_map = 'auto'\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=device_map,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True, padding_side='left', use_fast=False)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "print('Loaded model in 4-bit with bitsandbytes ✅')"
      ],
      "id": "bnb_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        "\n",
        "generate_text(...)Puts model in evaluation mode (not training).\n",
        "\n",
        "Tokenizes the prompt into tensors.\n",
        "\n",
        "Creates a generation configuration:\n",
        "\n",
        "do_sample=True → random sampling instead of always picking most likely token.\n",
        "\n",
        "temperature=0.7 → balances creativity vs determinism.\n",
        "\n",
        "top_p=0.9 → nucleus sampling (choose from top 90% probability mass).\n",
        "\n",
        "Stops at EOS token, pads with EOS if needed.\n",
        "\n",
        "Runs model generation (model.generate).\n",
        "\n",
        "Decodes tokens back to readable text.\n",
        " Basically: Runs the model to generate natural language output for a given input text.\n",
        "\n",
        "format_dialogsum_prompt(...)\n",
        "\n",
        "Prepares a standardized prompt for the DialogSum dataset.\n",
        "\n",
        "Takes raw conversation text and wraps it into an instruction style prompt:\n",
        "\n",
        "Instruct: Summarize the following conversation.\n",
        "<dialogue here>\n",
        "Output:\n",
        "\n",
        "\n",
        "This way, the model clearly understands the task.\n",
        "\n",
        "In short: These helper functions allow you to (1) generate text with controlled randomness, and (2) format conversations into a prompt that the model can summarize."
      ],
      "metadata": {
        "id": "gLZkeu98BEgT"
      },
      "id": "gLZkeu98BEgT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gen_helper"
      },
      "source": [
        "def generate_text(model, tokenizer, prompt, max_new_tokens=MAX_NEW_TOKENS, temperature=0.7, top_p=0.9):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "    gen_cfg = GenerationConfig(\n",
        "        do_sample=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, generation_config=gen_cfg)\n",
        "    return tokenizer.batch_decode(out, skip_special_tokens=True)\n",
        "\n",
        "def format_dialogsum_prompt(dialogue_text):\n",
        "    return f\"Instruct: Summarize the following conversation.\\n{dialogue_text}\\nOutput:\\n\""
      ],
      "id": "gen_helper",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Picks one example dialogue + its human-written summary.\n",
        "\n",
        "Formats the conversation into a prompt.\n",
        "\n",
        "Runs the base model (zero-shot, no fine-tuning).\n",
        "\n",
        "Prints:\n",
        "\n",
        "The input prompt\n",
        "\n",
        "The human reference summary\n",
        "\n",
        "The model’s generated summary"
      ],
      "metadata": {
        "id": "2-W4K4rADWMA"
      },
      "id": "2-W4K4rADWMA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zero_shot"
      },
      "source": [
        "idx = 0\n",
        "sample_dialogue = ds['test'][idx]['dialogue'] if 'test' in ds else ds['validation'][idx]['dialogue']\n",
        "sample_summary = ds['test'][idx]['summary'] if 'test' in ds else ds['validation'][idx]['summary']\n",
        "prompt = format_dialogsum_prompt(sample_dialogue)\n",
        "base_out = generate_text(base_model, tokenizer, prompt)[0]\n",
        "print('-'*80)\n",
        "print('INPUT PROMPT:\\n', prompt)\n",
        "print('-'*80)\n",
        "print('BASELINE HUMAN SUMMARY:\\n', sample_summary)\n",
        "print('-'*80)\n",
        "print('MODEL (zero-shot) OUTPUT:\\n', base_out.split('Output:\\n')[-1])"
      ],
      "id": "zero_shot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Builds instruction-style prompts (with special markers).\n",
        "\n",
        "Figures out model’s maximum sequence length.\n",
        "\n",
        "Tokenizes those formatted prompts so the model can be trained on them.\n",
        "\n",
        "Purpose: To transform raw dataset (dialogue + summary) → into structured, tokenized input/output pairs for supervised fine-tuning."
      ],
      "metadata": {
        "id": "wDacO4v5Glkv"
      },
      "id": "wDacO4v5Glkv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preprocess"
      },
      "source": [
        "INTRO_BLURB = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.'\n",
        "INSTRUCTION_KEY = '### Instruct: Summarize the below conversation.'\n",
        "RESPONSE_KEY = '### Output:'\n",
        "END_KEY = '### End'\n",
        "\n",
        "def create_prompt_formats(sample):\n",
        "    blurb = f\"\\n{INTRO_BLURB}\"\n",
        "    instruction = f\"{INSTRUCTION_KEY}\"\n",
        "    input_context = f\"{sample['dialogue']}\" if sample.get('dialogue') else None\n",
        "    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n",
        "    end = f\"{END_KEY}\"\n",
        "    parts = [p for p in [blurb, instruction, input_context, response, end] if p]\n",
        "    sample['text'] = '\\n\\n'.join(parts)\n",
        "    return sample\n",
        "\n",
        "def get_max_length(model):\n",
        "    for k in ['n_positions','max_position_embeddings','seq_length']:\n",
        "        v = getattr(model.config, k, None)\n",
        "        if v:\n",
        "            return int(v)\n",
        "    return min(int(getattr(model.config, 'max_position_embeddings', 2048)), 2048)\n",
        "\n",
        "def tokenize_batch(batch, tokenizer, max_length):\n",
        "    return tokenizer(batch['text'], max_length=max_length, truncation=True)"
      ],
      "id": "preprocess",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This code:\n",
        "\n",
        "Gets the model’s max sequence length.\n",
        "\n",
        "Loops over dataset splits.\n",
        "\n",
        "Formats each sample into instruction-response style text.\n",
        "\n",
        "Tokenizes into IDs.\n",
        "\n",
        "Removes too-long samples.\n",
        "\n",
        "Saves everything into processed (a fully training-ready dataset).\n",
        "\n",
        "End result:\n",
        "processed = a clean, tokenized dataset, ready to feed into HuggingFace’s Trainer."
      ],
      "metadata": {
        "id": "4LNxpCXSIiNV"
      },
      "id": "4LNxpCXSIiNV"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "build_ds"
      },
      "source": [
        "max_len = get_max_length(base_model)\n",
        "print('Using max_length =', max_len)\n",
        "processed = DatasetDict()\n",
        "for split in ds.keys():\n",
        "    cur = ds[split].map(create_prompt_formats)\n",
        "    cur = cur.map(partial(tokenize_batch, tokenizer=tokenizer, max_length=max_len), batched=True,\n",
        "                  remove_columns=[c for c in ds[split].column_names if c not in ['text']])\n",
        "    cur = cur.filter(lambda s: len(s['input_ids']) < max_len)\n",
        "    processed[split] = cur\n",
        "    print(split, processed[split])"
      ],
      "id": "build_ds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Adjusts the 4-bit quantized model so it’s safe for fine-tuning (prepare_model_for_kbit_training).\n",
        "\n",
        "Turns on gradient checkpointing to save memory.\n",
        "\n",
        "Prepares the base model for QLoRA training (parameter-efficient fine-tuning on limited GPU resources"
      ],
      "metadata": {
        "id": "SVQYP1BtJwlk"
      },
      "id": "SVQYP1BtJwlk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prep_kbit"
      },
      "source": [
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "base_model.gradient_checkpointing_enable()\n",
        "print('Model prepared for QLoRA training ✅')"
      ],
      "id": "prep_kbit",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Defines the LoRA adapter configuration (where and how they’re added).\n",
        "\n",
        "Inserts LoRA adapters into the base quantized model.\n",
        "\n",
        "Prints how many parameters will actually be trained (usually <1% of the full model)."
      ],
      "metadata": {
        "id": "bhxTMo_mLlgc"
      },
      "id": "bhxTMo_mLlgc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lora_cfg"
      },
      "source": [
        "lora_cfg = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['q_proj','k_proj','v_proj','dense'],  # works well for Phi-2\n",
        "    bias='none',\n",
        "    lora_dropout=0.05,\n",
        "    task_type='CAUSAL_LM',\n",
        ")\n",
        "peft_model = get_peft_model(base_model, lora_cfg)\n",
        "\n",
        "def print_trainable_parameters(m):\n",
        "    trainable = 0; total = 0\n",
        "    for _, p in m.named_parameters():\n",
        "        num = p.numel(); total += num\n",
        "        if p.requires_grad: trainable += num\n",
        "    print(f'Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.4f}%)')\n",
        "\n",
        "print_trainable_parameters(peft_model)"
      ],
      "id": "lora_cfg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Configures how training will run (steps, optimizer, logging, evaluation).\n",
        "\n",
        "Prepares a data collator for batching tokenized text.\n",
        "\n",
        "Creates a Hugging Face Trainer with model + datasets.\n",
        "\n",
        "Disables caching so gradient checkpointing works.\n",
        "\n",
        "At this point → the model is fully ready to start training with:"
      ],
      "metadata": {
        "id": "EK_UDJ4fMCSg"
      },
      "id": "EK_UDJ4fMCSg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trainer_args"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=PER_DEVICE_BATCH,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    max_steps=TRAIN_STEPS,\n",
        "    warmup_steps=1,\n",
        "    logging_steps=25,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    optim='paged_adamw_8bit',\n",
        "    group_by_length=True,\n",
        "    report_to='none',\n",
        "    gradient_checkpointing=True,\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=args,\n",
        "    train_dataset=processed['train'],\n",
        "    eval_dataset=processed.get('validation', None),\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "peft_model.config.use_cache = False\n",
        "print('Trainer ready ✅')"
      ],
      "id": "trainer_args",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "train"
      },
      "source": [
        "trainer.train()"
      ],
      "id": "train",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Creates a save directory for the LoRA adapter.\n",
        "\n",
        "Saves only the LoRA-trained weights (tiny, efficient).\n",
        "\n",
        "Saves the tokenizer so the model can be used later.\n",
        "\n",
        "Prints the save path for confirmation."
      ],
      "metadata": {
        "id": "sEv8MFdBNuag"
      },
      "id": "sEv8MFdBNuag"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "save"
      },
      "source": [
        "save_dir = os.path.join(OUTPUT_DIR, 'adapter')\n",
        "peft_model.save_pretrained(save_dir)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print('Saved LoRA adapter to:', save_dir)"
      ],
      "id": "save",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell:\n",
        "\n",
        "Reloads the original pretrained model with 4-bit quantization.\n",
        "\n",
        "Loads and attaches the fine-tuned LoRA adapter weights.\n",
        "\n",
        "Produces a final model (ft_model) ready for inference."
      ],
      "metadata": {
        "id": "xshTykFvOEr8"
      },
      "id": "xshTykFvOEr8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reload"
      },
      "source": [
        "reload_base = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map='auto',\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "from peft import PeftModel\n",
        "ft_model = PeftModel.from_pretrained(reload_base, save_dir, is_trainable=False)\n",
        "print('Adapter loaded for inference ✅')"
      ],
      "id": "reload",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell tests your fine-tuned LoRA adapter on a sample dialogue.\n",
        "\n",
        "You can compare:\n",
        "\n",
        "The raw input (prompt)\n",
        "\n",
        "The human-written summary (sample_summary)\n",
        "\n",
        "The model-generated summary (ft_out)"
      ],
      "metadata": {
        "id": "Ps4On7OfOXEU"
      },
      "id": "Ps4On7OfOXEU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qual_eval"
      },
      "source": [
        "prompt = format_dialogsum_prompt(sample_dialogue)\n",
        "ft_out = generate_text(ft_model, tokenizer, prompt)[0]\n",
        "print('-'*80)\n",
        "print('INPUT PROMPT:\\n', prompt)\n",
        "print('-'*80)\n",
        "print('BASELINE HUMAN SUMMARY:\\n', sample_summary)\n",
        "print('-'*80)\n",
        "print('QLoRA (fine-tuned) OUTPUT:\\n', ft_out.split('Output:\\n')[1].split('###')[0])"
      ],
      "id": "qual_eval",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This cell evaluates how well the baseline and fine-tuned models summarize dialogues compared to humans.\n",
        "\n",
        "ROUGE gives a numeric score; higher values mean closer to human summaries.\n",
        "\n",
        "This is how you measure the effect of QLoRA fine-tuning."
      ],
      "metadata": {
        "id": "Wi-duI5cOknG"
      },
      "id": "Wi-duI5cOknG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rouge_eval"
      },
      "source": [
        "import evaluate\n",
        "rouge = evaluate.load('rouge')\n",
        "N = min(10, len(processed.get('test', processed['validation'])))\n",
        "dlg = (processed.get('test', processed['validation']))[:N]['text']\n",
        "human_refs = (ds.get('test', ds['validation']))[:N]['summary']\n",
        "\n",
        "orig_summaries, ft_summaries = [], []\n",
        "for i in range(N):\n",
        "    # derive the dialogue back from text to feed into prompt (simple heuristic)\n",
        "    # For reliable eval, you should keep the raw dialogs separately.\n",
        "    raw_dialogue = ds.get('test', ds['validation'])[i]['dialogue']\n",
        "    p = format_dialogsum_prompt(raw_dialogue)\n",
        "    o = generate_text(reload_base, tokenizer, p)[0]\n",
        "    f = generate_text(ft_model, tokenizer, p)[0]\n",
        "    orig_summaries.append(o.split('Output:\\n')[-1])\n",
        "    ft_summaries.append(f.split('Output:\\n')[-1])\n",
        "\n",
        "orig_scores = rouge.compute(predictions=orig_summaries, references=human_refs, use_stemmer=True)\n",
        "ft_scores = rouge.compute(predictions=ft_summaries, references=human_refs, use_stemmer=True)\n",
        "print('\\nOriginal model ROUGE:', orig_scores)\n",
        "print('Fine-tuned model ROUGE:', ft_scores)"
      ],
      "id": "rouge_eval",
      "execution_count": null,
      "outputs": []
    }
  ]
}